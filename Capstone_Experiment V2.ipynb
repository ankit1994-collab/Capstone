{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/Jessicawwww/Capstone-Experiments/blob/main/Capstone_Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18782,"status":"ok","timestamp":1680665854146,"user":{"displayName":"Jingyi Wu","userId":"11195917611240117020"},"user_tz":240},"id":"Hkngu1BpfzAy","outputId":"b16e228c-81d6-432d-edfe-4e10ba4da61d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["#Mounting Google Drive onto virtual environment\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# #Installing Base Packages\n","# !pip3 install utils\n","# !pip3 install numpy==1.17.0\n","# !pip3 install scipy==1.4.1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":610,"status":"ok","timestamp":1680665857914,"user":{"displayName":"Jingyi Wu","userId":"11195917611240117020"},"user_tz":240},"id":"pdfcG0gGf4X3","outputId":"52e209c8-e920-46f1-f8fd-5b6a936b66f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1DzBEX3L6TIcOWVCpjtc2tmWd8buAK-zO/Capstone TCS/final\n"]}],"source":["#/content/drive/My Drive/ gets to your drive. \n","%cd /content/drive/My Drive/Capstone TCS/final"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_P5ckwHBgdzh"},"outputs":[],"source":["# %ls"]},{"cell_type":"markdown","metadata":{"id":"n5JPQfDGfWpt"},"source":["## EDA (to be continued)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7483,"status":"ok","timestamp":1680665728131,"user":{"displayName":"Ankit Kumar","userId":"10857990373911935083"},"user_tz":240},"id":"VjerOvl1nVkh","outputId":"127476a1-af4d-420c-886b-7bbbfbf05805"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow_text\n","  Downloading tensorflow_text-2.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow_text) (2.12.0)\n","Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow_text) (0.13.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.14.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (23.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (16.0.0)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.0)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.22.4)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.32.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (3.3.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.53.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (23.3.3)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.4.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (67.6.1)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (3.20.3)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.6.3)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.7)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (4.5.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (3.8.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.16.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.40.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.10.1)\n","Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.0.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.7.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.2.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.17.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.4.3)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (6.1.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.0.12)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.2.2)\n","Installing collected packages: tensorflow_text\n","Successfully installed tensorflow_text-2.12.0\n"]}],"source":["!pip install tensorflow_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"maYLu_YHnNNo"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","import tempfile\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HNsqwlrfWpv"},"outputs":[],"source":["import pandas as pd\n","import glob\n","\n","schema = {\n","    'song_hotttnesss': float,\n","    'artist_familiarity': float,\n","    'artist_hotttnesss': float,\n","    'artist_id': str,\n","    'artist_latitude': float,\n","    'artist_location': str,\n","    'artist_longitude': float,\n","    'artist_name': str,\n","    'title': str,\n","    'danceability': float,\n","    'duration': float,\n","    'end_of_fade_in': float,\n","    'energy': float,\n","    'key': float,\n","    'key_confidence': float,\n","    'loudness': float,\n","    'mode': float,\n","    'mode_confidence': float,\n","    'start_of_fade_out': float,\n","    'tempo': float,\n","    'time_signature': float,\n","    'time_signature_confidence': float,\n","    'artist_terms': str,\n","    'artist_terms_freq': str,\n","    'artist_terms_weight': str,\n","    'year': float\n","}\n","\n","# read each CSV file into a separate DataFrame, and concatenate them\n","cols=list(schema.keys())\n","temp=[]\n","# for csv_file in glob.glob('/Users/olivia/Downloads/final/*.csv'):\n","for csv_file in glob.glob('*.csv'):\n","    temp_df = pd.read_csv(csv_file,names=cols)\n","    temp.append(temp_df)\n","    \n","df = pd.concat(temp, ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BIuvYVOMfWpw"},"outputs":[],"source":["len(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XpNi1Ap_fWpw"},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NyeBqmGbfWpx"},"outputs":[],"source":["df.danceability.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaR_hRoFfWpx"},"outputs":[],"source":["df.energy.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zhzw9dvzfWpx"},"outputs":[],"source":["# drop empty features\n","df.drop(['danceability','energy'],axis=1,inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mA60R0P6fWpx"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNdA4erQfWpy","scrolled":false},"outputs":[],"source":["for col in df.columns:\n","    num_nans = df[col].isnull().sum()\n","    print(f\"Number of NaN values in column '{col}': {num_nans}\")"]},{"cell_type":"markdown","metadata":{"id":"80hVAzSOfWpy"},"source":["We define a song to be popular if its song_hotttnesss value is above average,and assign a label called popularity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0hXMk7skfWpy"},"outputs":[],"source":["import numpy as np\n","pop_avg=np.mean(df['song_hotttnesss'])\n","df['popularity']= df.song_hotttnesss.map(lambda x: 1 if x > pop_avg else 0)"]},{"cell_type":"markdown","metadata":{"id":"QcmMzvdgfWpy"},"source":["**Here is one way to split the data if you want a pre/post deploy differentiation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykRF9GQqfWpy"},"outputs":[],"source":["# define the three columns to split by\n","split_cols = ['artist_latitude', 'artist_longitude']\n","\n","\n","# Split the dataframe into two parts based on the null values in split_cols\n","df_notnull = df[df[split_cols].notnull().all(axis=1)]\n","df_null = df[~df[split_cols].notnull().all(axis=1)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1VO0TKrUfWpy"},"outputs":[],"source":["len(df_null)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1Y3JW2hfWpz"},"outputs":[],"source":["len(df_notnull)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NM9QLAUkfWpz"},"outputs":[],"source":["df_notnull.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LzqwJ3ZmfWpz"},"outputs":[],"source":["labels = ['artist_familiarity', 'artist_hotttnesss', 'duration', 'end_of_fade_in', 'key', \n","            'key_confidence', 'loudness', 'mode', 'mode_confidence', 'start_of_fade_out', 'tempo', \n","            'time_signature', 'time_signature_confidence','year', 'artist_latitude', 'artist_longitude','popularity']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CUlIGocrfWpz"},"outputs":[],"source":["#Drop 'artist_familiarity' NA\n","# pre_data = df_notnull[labels]\n","# post_data = df_null[labels]\n","# post_data[['artist_latitude', 'artist_longitude']] = post_data[['artist_latitude', 'artist_longitude']].fillna(0)\n","# pre_data = pre_data.dropna()\n","# post_data = post_data.dropna()\n","\n","post_data = df_notnull[labels]\n","pre_data = df_null[labels]\n","pre_data[['artist_latitude', 'artist_longitude']] = pre_data[['artist_latitude', 'artist_longitude']].fillna(0)\n","pre_data = pre_data.dropna()\n","post_data = post_data.dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjMPW6qtfWpz"},"outputs":[],"source":["len(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3RXfKWApthH"},"outputs":[],"source":["import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ANI6PuGfWpz","scrolled":false},"outputs":[],"source":["for i in range(0,16):\n","    plt.figure(figsize=(3,2))\n","    plt.hist(pre_data[labels[i]], bins = 100, alpha = 0.5)\n","    plt.hist(post_data[labels[i]], bins = 100, alpha = 0.5)\n","    plt.xlabel('Values')\n","    plt.ylabel('Frequency')\n","    plt.title('Column '+ str(i+1))\n","    plt.legend(['prev', 'latter'])\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8V6g81H4fWpz"},"outputs":[],"source":["#Label distribution\n","plt.figure(figsize=(3,2))\n","plt.hist(pre_data['popularity'], bins = 100, alpha = 0.5)\n","plt.hist(post_data['popularity'], bins = 100, alpha = 0.5)\n","plt.xlabel('Values')\n","plt.ylabel('Frequency')\n","plt.title('Column label')\n","plt.legend(['prev', 'latter'])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGBqdZQKlP6w"},"outputs":[],"source":["# !pip3 install -U scikit-learn scipy matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5c-g0HnlP6w"},"outputs":[],"source":["# !pip install --upgrade pip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3ua_u4lfWp0"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import neighbors\n","from sklearn.metrics import mean_squared_error \n","from math import sqrt\n","import joblib"]},{"cell_type":"markdown","metadata":{"id":"zZnDqfxnfWp0"},"source":["## Training demo"]},{"cell_type":"markdown","metadata":{"id":"NQRE16A8fWp0"},"source":["We define a song to be popular if its song_hotttnesss value is above average,"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tsf7vif3fWp0"},"outputs":[],"source":["numeric_features= ['artist_familiarity', 'artist_hotttnesss', 'duration', 'end_of_fade_in', 'key', \n","            'key_confidence', 'loudness', 'mode', 'mode_confidence', 'start_of_fade_out', 'tempo', \n","            'time_signature', 'time_signature_confidence','year', 'artist_latitude', 'artist_longitude']\n","label= ['popularity']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UjHPCbUfWp0"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import confusion_matrix\n","\n","# Split data into training and testing sets\n","df_sample=pre_data[numeric_features+label].dropna()\n","X = df_sample[numeric_features]\n","y = df_sample[label]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","# scaler = MinMaxScaler()\n","# X_train = scaler.fit_transform(X_train)\n","# X_test = scaler.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"luvDFhbKtFj6"},"outputs":[],"source":["# Instantiate a Random Forest Classifier with 100 estimators\n","rf = RandomForestClassifier(random_state=42)\n","\n","# Fit the model to the training data\n","rf.fit(X_train, y_train)\n","\n","# Predict the labels of the test data\n","y_pred = rf.predict(X_test)\n","\n","# Compute accuracy score\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Random Forest Classifier Accuracy:\", accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WnPEm0PlP6z"},"outputs":[],"source":["# Calculate the confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","print(\"Confusion Matrix\")\n","print(cm)\n","print(\"Model benchmark\")\n","benchmark = accuracy\n","print(benchmark)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2y_J2RmEOdjq"},"outputs":[],"source":["Model_directory = tempfile.gettempdir()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWOrjEaFOg6w"},"outputs":[],"source":["Model_directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTz202v0OqaC"},"outputs":[],"source":["version = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZxsYlzvOx3r"},"outputs":[],"source":["export_path = os.path.join(Model_directory,str(version))\n","print('export_path = {}\\n'.format(export_path))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ciEisCSfWp0"},"outputs":[],"source":["#save model\n","import joblib\n","joblib.dump(rf, \"./deployed_model.joblib\")\n","rf.save(export_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7jRFgNO7myd4"},"outputs":[],"source":["import joblib\n","deployed_model = joblib.load(\"./deployed_model.joblib\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NceCqaoVflh6"},"outputs":[],"source":["import pickle\n","pickle.dump(deployed_model, open(\"rfc.pkl\", 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27309,"status":"ok","timestamp":1680667686873,"user":{"displayName":"Jingyi Wu","userId":"11195917611240117020"},"user_tz":240},"id":"mjN2XCoVmOzF","outputId":"dd9c677f-29f8-48b6-a0f7-d4f386eff8ed"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.2.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 1.2.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomizedSearchCV from version 1.2.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]}],"source":["retrained_model = joblib.load(\"./retrained_model.joblib\")\n","pickle.dump(retrained_model, open(\"retrained_model.pkl\", 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAHHKmt8n6_l"},"outputs":[],"source":["deployed_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQL5cwyxrFTA"},"outputs":[],"source":["print(len(pre_data))\n","print(len(post_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3xxlqc6s-1K"},"outputs":[],"source":["# Predict the labels of the test data\n","y_pred_tr = deployed_model.predict(X_train)\n","y_pred_test = deployed_model.predict(X_test)\n","\n","# Compute accuracy score\n","train_score = accuracy_score(y_train, y_pred_tr) * 100\n","print(f\"Train accuracy score: {train_score:.2f}%\")\n","test_score = accuracy_score(y_test, y_pred_test) * 100\n","print(f\"Test accuracy score: {test_score:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PtuLtwWoujF2"},"outputs":[],"source":["np.unique(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJRkn8HyrFMP"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","cm = confusion_matrix(y_test, y_pred_test)\n","disp = ConfusionMatrixDisplay(\n","    confusion_matrix=cm,\n","    display_labels=[0,1]\n",")\n","\n","\n","# NOTE: Fill all variables here with default values of the plot_confusion_matrix\n","fig, ax = plt.subplots(figsize=(10, 10))\n","disp = disp.plot(xticks_rotation='vertical', ax=ax, cmap=\"summer\")\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7Uc09XIxaj-"},"outputs":[],"source":["pd.DataFrame(classification_report(y_test, y_pred_test, output_dict=True)).T"]},{"cell_type":"markdown","metadata":{"id":"1pzuu3SHbZIY"},"source":["## Change of Data Schema"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vlNh203ubYdq"},"outputs":[],"source":["def test_data_schema(pre_data, post_data):\n","    assert set(pre_data.columns) == set(post_data.columns),\"Data Schema shifted.\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SiCOwcTlP62"},"outputs":[],"source":["test_data_schema(pre_data, post_data)"]},{"cell_type":"markdown","metadata":{"id":"RScHVuPVlP62"},"source":["This indicates that there is no data schema change for our dataset."]},{"cell_type":"markdown","metadata":{"id":"HIZW0S1lKp6L"},"source":["## Amount of new data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-AC15KUwKspe"},"outputs":[],"source":["new_data_func = lambda pre, post: True if len(post)>len(pre) else False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOvNIZNWK-eG"},"outputs":[],"source":["new_data_func(pre_data, post_data)"]},{"cell_type":"markdown","metadata":{"id":"SZ3FjsivlP63"},"source":["This indicates that post data has more rows than pre data. This is expected as need to retrain the model with post data added."]},{"cell_type":"markdown","metadata":{"id":"3OyVuc5vxk8J"},"source":["## Concept Drift: shift in relationship between X and y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4fN_xnTzChl"},"outputs":[],"source":["print(len(post_data), len(pre_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AnOGlegCxm7G"},"outputs":[],"source":["X_post = post_data[numeric_features]\n","y_post = post_data[label]\n","y_pred_post = deployed_model.predict(X_post)\n","# X_post = scaler.transform(X_post)\n","# Compute accuracy score\n","post_score = accuracy_score(y_post, y_pred_post) * 100\n","print(f\"Post data accuracy score: {post_score:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8dGgh8mzNL3"},"outputs":[],"source":["print(len(X_train), len(X_test), len(X_post))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jrkk0eBeyhup"},"outputs":[],"source":["cm_post = confusion_matrix(y_post, y_pred_post)\n","disp_post = ConfusionMatrixDisplay(\n","    confusion_matrix=cm_post,\n","    display_labels=[0,1]\n",")\n","\n","\n","# NOTE: Fill all variables here with default values of the plot_confusion_matrix\n","fig, ax = plt.subplots(figsize=(8, 8))\n","disp_post = disp_post.plot(xticks_rotation='vertical', ax=ax, cmap=\"summer\")\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1LYzAzvbzPak"},"outputs":[],"source":["pd.DataFrame(classification_report(y_post, y_pred_post, output_dict=True)).T"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwQs5J-Xz6Bo"},"outputs":[],"source":["from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","def post_score(y_true, y_pred, threshold, metric_num):\n","  if metric_num == 1:\n","    return True if f1_score(y_true, y_pred) < threshold else False\n","  elif metric_num == 2:\n","    return True if accuracy_score(y_true, y_pred) < threshold else False\n","  elif metric_num == 3:\n","    return True if precision_score(y_true, y_pred) < threshold else False\n","  else:\n","    return True if recall_score(y_true, y_pred) < threshold else False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEEmYhYBXaZr"},"outputs":[],"source":["## test case\n","y_true = [0, 1, 1, 0, 1, 1]\n","y_pred = [0, 1, 1, 0, 0, 1]\n","threshold = 0.7151669958566613\n","post_score(y_true, y_pred, threshold, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fy0rZvY108y0"},"outputs":[],"source":["new_f1 = f1_score(y_test, y_pred_test)*(1-0.1)\n","print(new_f1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1LoVHvj1R1L"},"outputs":[],"source":["post_score(y_post, y_pred_post, new_f1, 2)"]},{"cell_type":"markdown","metadata":{"id":"VkVwm57hlP69"},"source":["This told us that the model on post data still performs well."]},{"cell_type":"markdown","metadata":{"id":"0AGl6K3vzYzx"},"source":["### Pairwise Correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_ZGII3plP6-"},"outputs":[],"source":["# !pip3 install seaborn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KXZ0ecCQmvtL"},"outputs":[],"source":["import seaborn as sns\n","X_post = post_data[numeric_features]\n","corr=X_post.corr()\n","mask = np.triu(np.ones_like(corr, dtype=bool))\n","f, ax = plt.subplots(figsize=(11, 9))\n","cmap = sns.diverging_palette(230, 20, as_cmap=True)\n","sns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n","            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJUzyZcjE5yl"},"outputs":[],"source":["corr = X_train.drop(['artist_latitude', 'artist_longitude'], axis=1).corr()\n","corr_post = X_post.drop(['artist_latitude', 'artist_longitude'], axis=1).corr()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"feeyy7E-IR2H"},"outputs":[],"source":["len(corr.to_numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ep7OU7S3FBdu"},"outputs":[],"source":["from numpy.linalg import norm\n","similaritys = []\n","for i in range(14):\n","  A = corr.to_numpy()[i]\n","  B = corr_post.to_numpy()[i]\n","  cosine = np.dot(A, B)/ (norm(A)*norm(B))\n","  similaritys.append(cosine)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yI72FKWCJAIH"},"outputs":[],"source":["# [s for s in similaritys if s < 0.99]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CLoLuTWjIP-U"},"outputs":[],"source":["def corr_cosine_similarity(corr1, corr2, threshold):\n","  \"\"\"compute the cosine similarity between two correlation coefficient dataframe\n","  corr1, corr2: dataframe\n","  threshold: for detecting cosine similarity between two arrays\n","  return if any cosine similarity falls below 0.99\n","  \"\"\"\n","  from numpy.linalg import norm\n","  A = corr.to_numpy()\n","  B = corr_post.to_numpy()\n","  for i in range(len(A)):\n","    a = A[i]\n","    b = B[i]\n","    cosine = np.dot(a, b)/ (norm(a)*norm(b))\n","    if cosine < threshold:\n","      return True\n","    return False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoKo5zDuKQYm"},"outputs":[],"source":["corr_cosine_similarity(corr, corr_post, 0.99)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6igTZaTQ8ewm"},"outputs":[],"source":["# c = corr.mask(np.eye(len(corr), dtype=bool)).abs()\n","# s = c.unstack()\n","# so = s.sort_values(kind=\"quicksort\", ascending=False)\n","# so[so>0.6]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"puAG43yX8dyn"},"outputs":[],"source":["# corr = X_train.drop(['artist_latitude', 'artist_longitude'], axis=1).corr()\n","# m = (corr.mask(np.eye(len(corr), dtype=bool)).abs() > 0.6).any()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYmF-oCt83YA"},"outputs":[],"source":["# raw = corr.loc[m, m]\n","# raw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxYOVPYY74Od"},"outputs":[],"source":["# raw_post = corr_post.loc[m, m]\n","# raw_post"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQLCngr9_zgT"},"outputs":[],"source":["# similarity = pairwise.cosine_similarity(raw, raw_post)\n","# similarity"]},{"cell_type":"markdown","metadata":{"id":"gsUdavmlfWp0"},"source":["## Covariate Shift: Shift in the independent variables. \n","### 1. KS-Test（two sample）\n","H0：two samples come from the same distribution.\n","\n","H1：two samples' distribution are not the same.\n","\n","If the p-value is less than .05, we reject the null hypothesis. We have sufficient evidence to say that the two sample datasets do not come from the same distribution."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gObzTbs4fWp0"},"outputs":[],"source":["X_prev = pre_data.drop(columns = label)\n","X_post = post_data.drop(columns = label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A2orHRLnfWp0"},"outputs":[],"source":["from scipy.stats import ks_2samp\n","#return True means the program need to retrain\n","#k: at least k number of features are not from the same distribution\n","#X_prev,X_latter: dataframe\n","def KS_test(X_prev, X_latter, k):\n","    p_value = {'reject':0, 'accept':0}\n","    for i in X_prev.columns:\n","        p = ks_2samp(np.array(X_prev[i]), np.array(X_latter[i])).pvalue\n","        if p < 0.05:\n","            p_value['reject'] +=1\n","        else:\n","            p_value['accept'] +=1\n","    if p_value['reject'] > k:\n","        return True\n","    else: \n","        return False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMVmfEoPfWp0"},"outputs":[],"source":["#EXP: need to be retained\n","KS_test(X_prev, X_post, 2)"]},{"cell_type":"markdown","metadata":{"id":"KQoE0i8efWp0"},"source":["## 2. PSI\n","Interpretation\n","\n","PSI < 0.1: no significant population change\n","\n","PSI < 0.2: moderate population change\n","\n","PSI >= 0.2: significant population change\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q73t1jgefWp0"},"outputs":[],"source":["#Reference: https://github.com/mwburke/population-stability-index/blob/master/psi.py\n","\n","def calculate_psi(expected, actual, buckettype='bins', buckets=10, axis=0):\n","    '''Calculate the PSI (population stability index) across all variables\n","    Args:\n","       expected: numpy matrix of original values\n","       actual: numpy matrix of new values, same size as expected\n","       buckettype: type of strategy for creating buckets, bins splits into even splits, quantiles splits into quantile buckets\n","       buckets: number of quantiles to use in bucketing variables\n","       axis: axis by which variables are defined, 0 for vertical, 1 for horizontal\n","    Returns:\n","       psi_values: ndarray of psi values for each variable\n","    Author:\n","       Matthew Burke\n","       github.com/mwburke\n","       worksofchart.com\n","    '''\n","\n","    def psi(expected_array, actual_array, buckets):\n","        '''Calculate the PSI for a single variable\n","        Args:\n","           expected_array: numpy array of original values\n","           actual_array: numpy array of new values, same size as expected\n","           buckets: number of percentile ranges to bucket the values into\n","        Returns:\n","           psi_value: calculated PSI value\n","        '''\n","\n","        def scale_range (input, min, max):\n","            input += -(np.min(input))\n","            input /= np.max(input) / (max - min)\n","            input += min\n","            return input\n","\n","\n","        breakpoints = np.arange(0, buckets + 1) / (buckets) * 100\n","\n","        if buckettype == 'bins':\n","            breakpoints = scale_range(breakpoints, np.min(expected_array), np.max(expected_array))\n","        elif buckettype == 'quantiles':\n","            breakpoints = np.stack([np.percentile(expected_array, b) for b in breakpoints])\n","\n","\n","        expected_percents = np.histogram(expected_array, breakpoints)[0] / len(expected_array)\n","        actual_percents = np.histogram(actual_array, breakpoints)[0] / len(actual_array)\n","\n","        def sub_psi(e_perc, a_perc):\n","            '''Calculate the actual PSI value from comparing the values.\n","               Update the actual value to a very small number if equal to zero\n","            '''\n","            if a_perc == 0:\n","                a_perc = 0.0001\n","            if e_perc == 0:\n","                e_perc = 0.0001\n","\n","            value = (e_perc - a_perc) * np.log(e_perc / a_perc)\n","            return(value)\n","\n","        psi_value = np.sum(sub_psi(expected_percents[i], actual_percents[i]) for i in range(0, len(expected_percents)))\n","\n","        return(psi_value)\n","\n","    if len(expected.shape) == 1:\n","        psi_values = np.empty(len(expected.shape))\n","    else:\n","        psi_values = np.empty(expected.shape[axis])\n","        print(psi_values)\n","    for i in range(0, len(psi_values)):\n","        if len(psi_values) == 1:\n","            psi_values = psi(expected, actual, buckets)\n","        elif axis == 0:\n","            psi_values[i] = psi(expected[:,i], actual[:,i], buckets)\n","        elif axis == 1:\n","            psi_values[i] = psi(expected[i,:], actual[i,:], buckets)\n","\n","    return(psi_values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2jOvZM6EfWp1"},"outputs":[],"source":["#return True means the program need to retrain\n","#k: at least k number of features has shift\n","def PSI_covariate(X_prev, X_latter, k):\n","    value = {'reject':0, 'accept':0}\n","    for i in X_prev.columns:\n","        v = calculate_psi(np.array(X_prev[i]), np.array(X_latter[i]))\n","        if v < 0.1:\n","            value['reject'] +=1\n","        else:\n","            value['accept'] +=1\n","    if value['reject'] > k:\n","        return True\n","    else: \n","        return False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YE6aveMafWp1"},"outputs":[],"source":["#EXP: need to be retained\n","PSI_covariate(X_prev, X_post, 2)"]},{"cell_type":"markdown","metadata":{"id":"bfJ4N1YcfWp1"},"source":["## 3. Z-Score\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XgjtrznfWp1"},"outputs":[],"source":["from scipy.stats import zscore\n","\n","def Z_score_covariate(X_prev, X_post, k):\n","    # calculate Z-scores of the source and target datasets\n","    z_pre = zscore(X_prev)\n","    z_post = zscore(X_post)\n","    \n","    p_value = {'reject':0, 'accept':0}\n","    for i in range(z_pre.shape[1]):\n","        # perform a Kolmogorov-Smirnov test on the Z-score distributions\n","        p = ks_2samp(np.array(z_pre[:,i]), np.array(z_post[:,i])).pvalue\n","        if p < 0.05:\n","            p_value['reject'] +=1\n","        else:\n","            p_value['accept'] +=1\n","    if p_value['reject'] > k:\n","        return True\n","    else: \n","        return False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CSSFat5mfWp1"},"outputs":[],"source":["#EXP: need to be retained\n","# Z_score_covariate(X_prev, X_post, 2)"]},{"cell_type":"markdown","metadata":{"id":"Y5sD0WBzfWp1"},"source":["## Prior Probability Shift: shift of the target variable\n","### 1. KS-Test（two sample）\n","H0：two samples come from the same distribution.\n","\n","H1：two samples' distribution are not the same.\n","\n","If the p-value is less than .05, we reject the null hypothesis. We have sufficient evidence to say that the two sample datasets do not come from the same distributio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evN0OCl0fWp1"},"outputs":[],"source":["y_prev = pre_data['popularity']\n","y_post = post_data['popularity']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdLoGktTfWp1"},"outputs":[],"source":["from scipy.stats import ks_2samp\n","#return True means the program need to retrain, check on the distribution of y\n","def KS_test_prior(y_prev, y_post):\n","    p = ks_2samp(np.array(y_prev), np.array(y_post)).pvalue\n","    if p < 0.05:\n","        return False\n","    else:\n","        return True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EwS9pvK8fWp1"},"outputs":[],"source":["KS_test_prior(y_prev, y_post)"]},{"cell_type":"markdown","metadata":{"id":"DJHEm2fDfWp1"},"source":["### 2. PSI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4t23t70KfWp1"},"outputs":[],"source":["#return True means the program need to retrain, check on the distribution of y\n","def PSI_prior(y_prev, y_post):\n","    value = calculate_psi(np.array(y_prev), np.array(y_post))\n","    #no significant population change\n","    if value < 0.1:\n","        return False\n","    else:\n","        return True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0fRDbXwHfWp2"},"outputs":[],"source":["#EXP: no need to be retained\n","PSI_prior(y_prev, y_post)"]},{"cell_type":"markdown","metadata":{"id":"ki-kh-R0fWp2"},"source":["### 3. Z-score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kHo8202_fWp2"},"outputs":[],"source":["from scipy.stats import zscore\n","\n","def Z_score_prior(y_prev, y_post):\n","    # calculate Z-scores of the source and target datasets\n","    z_pre = zscore(y_prev)\n","    z_post = zscore(y_post)\n","    \n","    p = ks_2samp(np.array(z_pre), np.array(z_post)).pvalue\n","\n","    if p < 0.05:\n","        return False\n","    else: \n","        return True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CrXvcgMFfWp2"},"outputs":[],"source":["#EXP: no need to be retained\n","Z_score_prior(y_prev, y_post)"]},{"cell_type":"markdown","source":["## Retraining Process\n"],"metadata":{"id":"qlDY6SIy4Z3-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"s5LMqwmWgbw3"},"outputs":[],"source":["merged_df = pre_data.append(post_data, ignore_index=True)\n","merged_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cn8HQxRDkm-k"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(merged_df.drop(columns = ['popularity']), merged_df['popularity'], test_size=0.3, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gpB4zyzknhQj"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","X_train_norm = scaler.fit_transform(X_train)\n","X_test_norm = scaler.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvFbAJM0hhtJ"},"outputs":[],"source":["## random forest classfier\n","rf = RandomForestClassifier(random_state=42)\n","# Fit the model to the training data\n","rf.fit(X_train, y_train)\n","# Predict the labels of the test data\n","y_pred_rf = rf.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"42JWZsfWlP7j"},"outputs":[],"source":["# !pip3 install xgboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_A_2xPlwhmYF"},"outputs":[],"source":["## Xgboost\n","from xgboost import XGBClassifier\n","xgb = XGBClassifier(random_state=42)\n","xgb.fit(X_train, y_train) \n","y_pred_xgb = rf.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cf9Q0vM8i_-K"},"outputs":[],"source":["## Gradient Boosting\n","from sklearn.ensemble import GradientBoostingClassifier\n","gbc = GradientBoostingClassifier(random_state=42).fit(X_train, y_train)\n","y_pred_gbc = gbc.predict(X_test)\n","# gbc.score(X_test, y_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2DHDccThomC"},"outputs":[],"source":["## KNN\n","# Use the KNN classifier to fit data:\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import cross_val_score\n","\n","knn = KNeighborsClassifier()\n","knn.fit(X_train_norm, y_train) \n","\n","# Predict y data with classifier: \n","y_predict_knn = knn.predict(X_test_norm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrhn5DCMnlLk"},"outputs":[],"source":["# cross validation - use acc \n","from sklearn.metrics import make_scorer\n","metric = accuracy_score\n","rf_scores = cross_val_score(rf, X_test, y_test, cv=5, scoring=make_scorer(metric))\n","xgb_scores = cross_val_score(xgb, X_test, y_test, cv=5, scoring=make_scorer(metric))\n","gbc_scores = cross_val_score(gbc, X_test, y_test, cv=5, scoring=make_scorer(metric))\n","knn_scores = cross_val_score(knn, X_test_norm, y_test, cv=5, scoring=make_scorer(metric))\n","\n","\n","print(\"Random Fores mean score:\", rf_scores.mean())\n","print(\"XGBoost mean score:\", xgb_scores.mean())\n","print(\"Gradient Boosting mean score:\", gbc_scores.mean())\n","print(\"KNN mean score:\", knn_scores.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V03mW32JlP8J"},"outputs":[],"source":["benchmark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1Kdv43borCN"},"outputs":[],"source":["scores = [rf_scores.mean(), xgb_scores.mean(), gbc_scores.mean(), knn_scores.mean()]\n","models = [rf, xgb, gbc, knn]\n","\n","best_score = max(scores)\n","best_model = models[scores.index(best_score)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8bj-J1YtlP8J"},"outputs":[],"source":["print(\"Best Mean Score\")\n","print(best_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1nimTXV_lP8J"},"outputs":[],"source":["a = print(best_model)\n","a"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"O4X2WlreoX3n","outputId":"3cae6bc7-630f-4acc-ba72-7fe51de30fcc"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n"]}],"source":["## use the best model and fine tuning the parameters\n","from sklearn.model_selection import RandomizedSearchCV\n","#the number of trees in the forest,\n","# the maximum depth of each tree, \n","# the minimum number of samples required to split a node, \n","# the minimum number of samples required to be at a leaf node, \n","# and the number of features\n","param_space = {'n_estimators': [50, 100, 200, 300],\n","              'max_depth': [None, 5, 10, 20],\n","              'min_samples_split': [2, 5, 10, 15],\n","              'min_samples_leaf': [1, 2, 4, 8],\n","              'max_features': ['sqrt', 'log2', None]}\n","\n","random_search = RandomizedSearchCV(\n","    estimator = best_model,\n","    param_distributions = param_space,\n","    n_jobs = 4,\n","    cv=5\n",")\n","\n","random_search.fit(X_train, y_train)\n","best_params = random_search.best_params_\n","best_params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TCw_qY5PlP8J"},"outputs":[],"source":["joblib.dump(random_search, \"./retrained_model.joblib\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RpcrQAykfSnI"},"outputs":[],"source":["import pickle\n","pickle.dump(, open(filename, 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ngn3RM39lP8K"},"outputs":[],"source":["# best_params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGYifc6SlP8K"},"outputs":[],"source":["# # Define the hyperparameter search space\n","# param_dist = {'n_estimators': [50, 100, 200, 300],\n","#               'max_depth': [None, 5, 10, 20],\n","#               'min_samples_split': [2, 5, 10, 15],\n","#               'min_samples_leaf': [1, 2, 4, 8],\n","#               'max_features': ['sqrt', 'log2', None]}\n","\n","# # Initialize a RandomForestClassifier\n","# rfc = RandomForestClassifier(random_state=42)\n","\n","# # Initialize a RandomizedSearchCV object with 10 iterations\n","# random_search = RandomizedSearchCV(rfc, param_distributions=param_dist, n_iter=10, cv=5)\n","\n","# # Fit the RandomizedSearchCV object to the data\n","# random_search.fit(X_train, y_train)\n","\n","# # Print the best hyperparameters found\n","# print(random_search.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICYcxRcNlP8K"},"outputs":[],"source":["# # Instantiate a Random Forest Classifier with 100 estimators\n","# rf_new = RandomForestClassifier(random_state=42)\n","\n","# # Fit the model to the training data\n","# rf_new.fit(X_train, y_train)\n","\n","# # Predict the labels of the test data\n","# y_pred = rf_new.predict(X_test)\n","\n","# # Compute accuracy score\n","# accuracy = accuracy_score(y_test, y_pred)\n","# print(\"Random Forest Classifier Accuracy:\", accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5OwKuC8lP8K"},"outputs":[],"source":["print (f'Train Accuracy: {random_search.score(X_train,y_train):.3f}')\n","print (f'Test Accuracy: {random_search.score(X_test,y_test):.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1lTbok2lP8L"},"outputs":[],"source":["benchmark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YllsP6HklP8L"},"outputs":[],"source":["#previous\n","# print (f'Train Accuracy: {random_search.score(X_train,y_train):.3f}')\n","# print (f'Test Accuracy: {random_search.score(X_test,y_test):.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZJrb289Jc5m"},"outputs":[],"source":["Model_directory = tempfile.gettempdir()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9qKirosJmuy"},"outputs":[],"source":["Model_directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6A3iJXjzJtJ7"},"outputs":[],"source":["version = 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPwFbnMBJ0JV"},"outputs":[],"source":["export_path = os.path.join(Model_directory,str(version))\n","print('export_path = {}\\n'.format(export_path))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbfzLR4NKiK7"},"outputs":[],"source":["##import tensorflow as tf\n","#model = joblib.load(\"./deployed_model.joblib\")\n","#tf.saved_model.save(model, '/content/drive/MyDrive/Capstone TCS/final/saved_model')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7L2frsF2Nt5Z"},"outputs":[],"source":["rf.save(export_path)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6_ZI4rkJmpg"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}